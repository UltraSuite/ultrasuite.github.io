{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"UltraSuite Repository A repository of ultrasound and acoustic data from child speech therapy sessions Ultrasuite is a collection of ultrasound and acoustic speech data from child speech therapy sessions. The current release includes three datasets, one from typically developing children and two from speech disordered children. Ultrasuite also includes a set of annotations, some manual and some automatically produced, and tools to process, transform and visualise the data. Read the Ultrasuite paper here! Ultrasuite also distributes the Tongue and Lips (TaL) corpus, a multi-speaker corpus of synchronised ultrasound images of the tongue and video images of lips. Read the TaL paper here! . Data There are three child speech datasets available in the repository: Ultrax Typically Developing - UXTD. A dataset of 58 typically developing children. See here for further details. Ultrax Speech Sound Disorders - UXSSD. A dataset of 8 children with speech sound disorders. See here for further details. UltraPhonix - UPX. A second dataset of children with speech sound disorders. The data was collected from 20 children. See here for further details. The Cleft Dataset - Cleft. A dataset of children with cleft lip and palate. The data is available for 29 children. See here for further details. And two datasets of adult speech: Tongue and Lips corpus - TaL1 . A single-speaker dataset with data of one professional voice talent, a male native speaker of English, over six recording sessions. Read about the TaL corpus . Tongue and Lips corpus - TaL80. A multi-speaker dataset with recording sessions of 81 native speakers of English without voice talent experience. Read about the TaL corpus . Code Ultrasuite Tools - Python library to process raw ultrasound data. Ultrasuite Kaldi - Recipes and other code to use UltraSuite data with the Kaldi Speech Recognition Toolkit . TaL Tools - Resources for the Tongue and Lips corpus. Contributing We welcome user contribution to Ultrasuite! We are hoping to keep Ultrasuite in active development with help from the community. All contributions will be given proper credit! There are various ways to participate: Contributing with data The current release of Ultrasuite has three datasets of ultrasound and audio from child speech and two datasets of adult speech, but we hope to include additional datasets in the future. If you'd like to share data that you collected, please get in touch with any member of the Ultrax Speech project . We are also happy to host any resources that you have generated for current Ultrasuite datasets (e.g. annotations, scores, tongue splines, etc). Note that even though data is available through UltraSuite, we encourage users to cite the original authors. Contributing with code To contribute with code or to help improve this documentation , please submit your changes with Pull Requests. Reporting issues To report any issues, you can use GitHub's Issue Tracker or you can contact any member of the Ultrax Speech project . Please submit any issues related to code in their respective repositories using Github's Issue Tracker. For issues found in the data, please contact us directly. Community Subscribe to the UltraSuite mailing list to get updates from the UltraSuite repository and to send and receive messages from other UltraSuite users. License Datasets from UltraSuite are distributed under Attribution-NonCommercial 4.0 Generic (CC BY-NC 4.0). Code is available under the Apache License v.2. Citations If using data from UXTD, UXSSD, or UPX , please cite the following paper: Eshky, A., Ribeiro, M. S., Cleland, J., Richmond, K., Roxburgh, Z., Scobbie, J., & Wrench, A. (2018) Ultrasuite: A repository of ultrasound and acoustic data from child speech therapy sessions . Proceedings of INTERSPEECH. Hyderabad, India. [ paper ] If using data from the Tongue and Lips (TaL) corpus , please cite the following paper: Ribeiro, M. S., Sanger, J., Zhang, J.-X., Eshky, A., Wrench, A., Richmond, K.,& Renals, S. (2021). TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging, audio, and lip videos. Proceedings of the IEEE Workshop on Spoken Language Technology (SLT). Shenzhen, China. [ paper ]","title":"Home"},{"location":"#ultrasuite-repository","text":"A repository of ultrasound and acoustic data from child speech therapy sessions Ultrasuite is a collection of ultrasound and acoustic speech data from child speech therapy sessions. The current release includes three datasets, one from typically developing children and two from speech disordered children. Ultrasuite also includes a set of annotations, some manual and some automatically produced, and tools to process, transform and visualise the data. Read the Ultrasuite paper here! Ultrasuite also distributes the Tongue and Lips (TaL) corpus, a multi-speaker corpus of synchronised ultrasound images of the tongue and video images of lips. Read the TaL paper here! .","title":"UltraSuite Repository"},{"location":"#data","text":"There are three child speech datasets available in the repository: Ultrax Typically Developing - UXTD. A dataset of 58 typically developing children. See here for further details. Ultrax Speech Sound Disorders - UXSSD. A dataset of 8 children with speech sound disorders. See here for further details. UltraPhonix - UPX. A second dataset of children with speech sound disorders. The data was collected from 20 children. See here for further details. The Cleft Dataset - Cleft. A dataset of children with cleft lip and palate. The data is available for 29 children. See here for further details. And two datasets of adult speech: Tongue and Lips corpus - TaL1 . A single-speaker dataset with data of one professional voice talent, a male native speaker of English, over six recording sessions. Read about the TaL corpus . Tongue and Lips corpus - TaL80. A multi-speaker dataset with recording sessions of 81 native speakers of English without voice talent experience. Read about the TaL corpus .","title":"Data"},{"location":"#code","text":"Ultrasuite Tools - Python library to process raw ultrasound data. Ultrasuite Kaldi - Recipes and other code to use UltraSuite data with the Kaldi Speech Recognition Toolkit . TaL Tools - Resources for the Tongue and Lips corpus.","title":"Code"},{"location":"#contributing","text":"We welcome user contribution to Ultrasuite! We are hoping to keep Ultrasuite in active development with help from the community. All contributions will be given proper credit! There are various ways to participate: Contributing with data The current release of Ultrasuite has three datasets of ultrasound and audio from child speech and two datasets of adult speech, but we hope to include additional datasets in the future. If you'd like to share data that you collected, please get in touch with any member of the Ultrax Speech project . We are also happy to host any resources that you have generated for current Ultrasuite datasets (e.g. annotations, scores, tongue splines, etc). Note that even though data is available through UltraSuite, we encourage users to cite the original authors. Contributing with code To contribute with code or to help improve this documentation , please submit your changes with Pull Requests. Reporting issues To report any issues, you can use GitHub's Issue Tracker or you can contact any member of the Ultrax Speech project . Please submit any issues related to code in their respective repositories using Github's Issue Tracker. For issues found in the data, please contact us directly.","title":"Contributing"},{"location":"#community","text":"Subscribe to the UltraSuite mailing list to get updates from the UltraSuite repository and to send and receive messages from other UltraSuite users.","title":"Community"},{"location":"#license","text":"Datasets from UltraSuite are distributed under Attribution-NonCommercial 4.0 Generic (CC BY-NC 4.0). Code is available under the Apache License v.2.","title":"License"},{"location":"#citations","text":"If using data from UXTD, UXSSD, or UPX , please cite the following paper: Eshky, A., Ribeiro, M. S., Cleland, J., Richmond, K., Roxburgh, Z., Scobbie, J., & Wrench, A. (2018) Ultrasuite: A repository of ultrasound and acoustic data from child speech therapy sessions . Proceedings of INTERSPEECH. Hyderabad, India. [ paper ] If using data from the Tongue and Lips (TaL) corpus , please cite the following paper: Ribeiro, M. S., Sanger, J., Zhang, J.-X., Eshky, A., Wrench, A., Richmond, K.,& Renals, S. (2021). TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging, audio, and lip videos. Proceedings of the IEEE Workshop on Spoken Language Technology (SLT). Shenzhen, China. [ paper ]","title":"Citations"},{"location":"community/","text":"Community Mailing list Subscribe to the UltraSuite mailing list to get updates from the UltraSuite repository and to send and receive messages from other UltraSuite users. The UltraSuite mailing list can be found at: http://lists.inf.ed.ac.uk/mailman/listinfo/ultrasuite To post to this list (subscribers only), send your email to ultrasuite@inf.ed.ac.uk If you ever want to unsubscribe or change your options (e.g. switch to or from digest mode, change your password, etc.), visit your subscription page at: http://lists.inf.ed.ac.uk/mailman/options/ultrasuite/yourname@yourmail You can also make such adjustments via email by sending a message to: ultrasuite-request@inf.ed.ac.uk with the word \u2018help\u2019 in the subject or body (don\u2019t include the quotes), and you will get back a message with instructions. Publications This is a list of work describing, using, or referencing the data available in the Ultrasuite repository. Database papers Ribeiro, M. S., Sanger, J., Zhang, J.-X., Eshky, A., Wrench, A., Richmond, K.,& Renals, S. (2021). TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging, audio, and lip videos. Proceedings of the IEEE Workshop on Spoken Language Technology (SLT). Shenzhen, China.[ Paper ] Eshky, A., Ribeiro, M. S., Cleland, J., Richmond, K., Roxburgh, Z., Scobbie, J., & Wrench, A. (2018) Ultrasuite: A repository of ultrasound and acoustic data from child speech therapy sessions . Proceedings of INTERSPEECH. Hyderabad, India. [ Paper ] Applications Csap\u00f3, T. G., & Xu, K. (2020). Quantification of Transducer Misalignment in Ultrasound Tongue Imaging . Proc. Interspeech.[ Paper ] [ Code ] Zhu, J., Styler, W., & Calloway, I. (2020) MTracker-a CNN-based tool for automatic tracking of tongue contours . UltraFest, Indiana, USA. [ Paper ] [ Code ] Zhu, J., Styler, W., & Calloway, I. (2019). A CNN-based tool for automatic tongue contour tracking in ultrasound images . arXiv preprint arXiv:1907.10210. [ Paper ] [ Code ] Eshky, A., Ribeiro, M. S., Richmond, K., & Renals, S. (2019). Synchronising audio and ultrasound by learning cross-modal embeddings . Proc. Interspeech. Graz, Austria. [ Paper ] [ Code ]. Ribeiro, M. S., Eshky, A., Richmond, K., & Renals, S. (2019). Ultrasound tongue imaging for diarization and alignment of child speech therapy sessions . Proc. Interspeech. Graz, Austria. [ Paper ] [ Code ] Ribeiro, M. S., Eshky, A., Richmond, K., & Renals, S. (2019). Speaker-independent classification of phonetic segments from raw ultrasound in child speech . In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1328-1332). [ Paper ]. Shahin, M., Ahmed, B., Smith, D. V., Duenser, A., & Epps, J. (2019). Automatic Screening of Children with Speech Sound Disorders Using Paralinguistic Features . In IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP) (pp. 1-5).[ Paper (not open access) ]. Shahin, M., Zafar, U., & Ahmed, B. (2019). The Automatic Detection of Speech Disorders in Children: Challenges, Opportunities, and Preliminary Results . IEEE Journal of Selected Topics in Signal Processing, 14(2), 400-412. [ Paper (not open access) ]. Ribeiro, M. S., Eshky, A., Richmond, K., & Renals, S. (2018). Towards robust word alignment of child speech therapy sessions . UK Speech, Dublin, Ireland. [ Abstract ] [ Poster ] References Mozaffari, M. H., Sankoff, D., & Lee, W. S. (2019). Ultrasound tongue contour extraction using BowNet network: A deep learning approach . In Proceedings of Meetings on Acoustics 178, ASA (Vol. 39, No. 1, p. 020001). Acoustical Society of America. [ Paper ] Mozaffari, M. H., Ratul, M., Rab, A., & Lee, W. S. (2019). Irisnet: Deep learning for automatic and real-time tongue contour tracking in ultrasound video data using peripheral vision . arXiv preprint arXiv:1911.03972. Hewer, A., Steiner, I., & Richmond, K. (2019). Analysis of coarticulation using EMA data with a statistical shape space model of the tongue . Studientexte zur Sprachkommunikation: Elektronische Sprachsignalverarbeitung 2019, 296-303. [ Paper ]. Scobbie, J. M., & Ma, J. (2019). Say again? Individual articulatory strategies for producing a clearly-spoken minimal pair wordlist . Proc. 19th ICPhS. [ Paper ] Publications list last update: November 2020","title":"Community"},{"location":"community/#community","text":"","title":"Community"},{"location":"community/#mailing-list","text":"Subscribe to the UltraSuite mailing list to get updates from the UltraSuite repository and to send and receive messages from other UltraSuite users. The UltraSuite mailing list can be found at: http://lists.inf.ed.ac.uk/mailman/listinfo/ultrasuite To post to this list (subscribers only), send your email to ultrasuite@inf.ed.ac.uk If you ever want to unsubscribe or change your options (e.g. switch to or from digest mode, change your password, etc.), visit your subscription page at: http://lists.inf.ed.ac.uk/mailman/options/ultrasuite/yourname@yourmail You can also make such adjustments via email by sending a message to: ultrasuite-request@inf.ed.ac.uk with the word \u2018help\u2019 in the subject or body (don\u2019t include the quotes), and you will get back a message with instructions.","title":"Mailing list"},{"location":"community/#publications","text":"This is a list of work describing, using, or referencing the data available in the Ultrasuite repository. Database papers Ribeiro, M. S., Sanger, J., Zhang, J.-X., Eshky, A., Wrench, A., Richmond, K.,& Renals, S. (2021). TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging, audio, and lip videos. Proceedings of the IEEE Workshop on Spoken Language Technology (SLT). Shenzhen, China.[ Paper ] Eshky, A., Ribeiro, M. S., Cleland, J., Richmond, K., Roxburgh, Z., Scobbie, J., & Wrench, A. (2018) Ultrasuite: A repository of ultrasound and acoustic data from child speech therapy sessions . Proceedings of INTERSPEECH. Hyderabad, India. [ Paper ] Applications Csap\u00f3, T. G., & Xu, K. (2020). Quantification of Transducer Misalignment in Ultrasound Tongue Imaging . Proc. Interspeech.[ Paper ] [ Code ] Zhu, J., Styler, W., & Calloway, I. (2020) MTracker-a CNN-based tool for automatic tracking of tongue contours . UltraFest, Indiana, USA. [ Paper ] [ Code ] Zhu, J., Styler, W., & Calloway, I. (2019). A CNN-based tool for automatic tongue contour tracking in ultrasound images . arXiv preprint arXiv:1907.10210. [ Paper ] [ Code ] Eshky, A., Ribeiro, M. S., Richmond, K., & Renals, S. (2019). Synchronising audio and ultrasound by learning cross-modal embeddings . Proc. Interspeech. Graz, Austria. [ Paper ] [ Code ]. Ribeiro, M. S., Eshky, A., Richmond, K., & Renals, S. (2019). Ultrasound tongue imaging for diarization and alignment of child speech therapy sessions . Proc. Interspeech. Graz, Austria. [ Paper ] [ Code ] Ribeiro, M. S., Eshky, A., Richmond, K., & Renals, S. (2019). Speaker-independent classification of phonetic segments from raw ultrasound in child speech . In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1328-1332). [ Paper ]. Shahin, M., Ahmed, B., Smith, D. V., Duenser, A., & Epps, J. (2019). Automatic Screening of Children with Speech Sound Disorders Using Paralinguistic Features . In IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP) (pp. 1-5).[ Paper (not open access) ]. Shahin, M., Zafar, U., & Ahmed, B. (2019). The Automatic Detection of Speech Disorders in Children: Challenges, Opportunities, and Preliminary Results . IEEE Journal of Selected Topics in Signal Processing, 14(2), 400-412. [ Paper (not open access) ]. Ribeiro, M. S., Eshky, A., Richmond, K., & Renals, S. (2018). Towards robust word alignment of child speech therapy sessions . UK Speech, Dublin, Ireland. [ Abstract ] [ Poster ] References Mozaffari, M. H., Sankoff, D., & Lee, W. S. (2019). Ultrasound tongue contour extraction using BowNet network: A deep learning approach . In Proceedings of Meetings on Acoustics 178, ASA (Vol. 39, No. 1, p. 020001). Acoustical Society of America. [ Paper ] Mozaffari, M. H., Ratul, M., Rab, A., & Lee, W. S. (2019). Irisnet: Deep learning for automatic and real-time tongue contour tracking in ultrasound video data using peripheral vision . arXiv preprint arXiv:1911.03972. Hewer, A., Steiner, I., & Richmond, K. (2019). Analysis of coarticulation using EMA data with a statistical shape space model of the tongue . Studientexte zur Sprachkommunikation: Elektronische Sprachsignalverarbeitung 2019, 296-303. [ Paper ]. Scobbie, J. M., & Ma, J. (2019). Say again? Individual articulatory strategies for producing a clearly-spoken minimal pair wordlist . Proc. 19th ICPhS. [ Paper ] Publications list last update: November 2020","title":"Publications"},{"location":"download/","text":"Download The complete data from the UltraSuite repository is available via the rsync command. All flags from this tool should be valid when syncing the UltraSuite repository. Sample data from the UltraSuite repository is available via Edinburgh DataShare . Download sample data Download of a very small subset of the UltraSuite repository can be achieve via Edinburgh DataShare . The downloadable datasets on DataShare contain a small sample from each of the three datasets in the UltraSuite repository, as well as their accompanying documentation. This is convenient for users who are unable or unwilling to download the repository using rsync or users that want a quick and easy way to look at the data. Windows users rsync does not come pre-installed in Windows machines. If you are a Windows user, we have prepared instructions on how to setup rsync on your computer: Installing rsync on Windows Before going through those instructions, feel free to explore the sample data from Edinburgh DataShare and determine if the UltraSuite repository fit your needs. Additionally, as a Windows user, you might be interested in using Articulate Assistant Advanced (AAA) for your work. After downloading, the Ultrasuite data can be imported into the AAA software (revision 219.x or higher) one speaker at a time using the \u201cFile|Import|ultrasuite\u2026\u201d dialogue. First create a new project or open an existing project. The speaker will be created as a client in that project and all the selected files will be imported. Using rsync rsync is a command-line tool that allows efficient and fast transfer and synchronisation of files across systems. This is usually pre-install on Linux or mac machines. These instructions assume you have the tool installed on your system, but do not assume a lot of experience with its usage. If you are familiar with rsync , you can skip ahead and merely glance at the typical commands. All flags commonly used with the tool should be applicable to this scenario. See the man page for further details. If you are not familiar with rsync , you should be aware that, if you have a version of the data on your disk, rsync will synchronise your local data with the latest version of UltraSuite. If there are changes to the UltraSuite repository, your data will be overwritten with those latest changes. A useful flag to use with the tool is --dry-run , which will simulate the synchronisation process. Because the repository is quite large ( 380 GB ), please make sure you have enough space on your disk. Alternatively, go through these instructions and download only a subset of the data for which you have space for. Setting up To get started, please create a directory on a hard drive where a large amount of space is available. Then, change your current directory onto that newly created one. On Linux or macOS, this can be achieved by running: mkdir UltraSuite cd UltraSuite Download an utterance Now that we have a place to store the data, we will download some sample data from the repository. This might be useful if you wish to get a small example to get familiar with the format of the data or test some code. We first download a single utterance. This is a set of four files: a waveform (.wav), a text file with the prompt text and recording date time (.txt), the raw ultrasound data (ult), and a text file with parameters for the ultrasound data (.param). Running the following command should sync the first utterance from speaker 01M in the Ultrax Typically Developing (UXTD) data set. rsync -av ultrasuite-rsync.inf.ed.ac.uk::ultrasuite/core-uxtd/core/01M/001* . The sample we've just downloaded should give you an idea of the type of data available in UltraSuite. Speaker 01M in the UXTD set has 121 utterances. You can download different samples by changing 001* to the corresponding utterance. If you'd like, you can get all of the data for speaker 01M. This corresponds to 1.9GB ! To do so, please run the following command rsync -av ultrasuite-rsync.inf.ed.ac.uk::ultrasuite/core-uxtd/core/01M . Download a data set You can download an entire data set by syncing its corresponding directory. For example, for the UXTD data set (82GB), you can run the following command: rsync -av ultrasuite-rsync.inf.ed.ac.uk::ultrasuite/core-uxtd . The same command can be used to download /core-uxssd and /core-upx . Please be aware that each dataset can be quite large: Dataset Size /core-uxtd 82GB /core-uxssd 110GB /core-upx 191GB The structure of each data set is identical and it follows the following pattern: ./core-uxtd /doc /core /01M /02M ... Please see the list of speakers of each of the datasets, if you are interested in downloading data from single speakers. Each speaker directory will contain four data types: waveforms ( .wav ), prompts ( .txt ), ultrasound data ( .ult ), and ultrasound parameters ( .param ) . To simulate the process, you can run rsync with the flag --dry-run and get an estimate for the amount of data. Download label data To download manual and automatically-generated labels for all three datasets (~80MB), please run: rsync -av ultrasuite-rsync.inf.ed.ac.uk::ultrasuite/labels-uxtd-uxssd-upx . You can get labels for each of the datasets by appending /uxtd , /uxssd , or /upx to the command above. There is also a documentation directory for the labels called /doc . Previous versions of label data are archived in the top-level directory archive . To download an earlier version of label data, please run something like: rsync -av ultrasuite-rsync.inf.ed.ac.uk::ultrasuite/archive/labels-uxtd-uxssd-upx-1.0.zip . The command above will download version 1.0 of labels. Replace the version number as appropriate. Download selected data types Using rsync , it is possible to download only specific data types by specifying the desired extension. For example, you can download waveforms ( .wav ), prompts ( .txt ), ultrasound data ( .ult ), or ultrasound parameters ( .param ) separately. To achieve this, the rsync command would look like: rsync -av --include=\"*/\" --include=\"*.wav\" --exclude=\"*\" ultrasuite-rsync.inf.ed.ac.uk::ultrasuite/core-uxtd . This command downloads all waveforms from the UXTD dataset. Note that the --include flags works together with the --exclude flag. It is used to include files that would otherwise be excluded. The way to interpret this command is that we first exclude all files with --exclude=\"*\" . Then we include all directories with --include=\"*/\" and all files ending with the desired extension with --include=\"*.wav\" . Additional include flags may specify other data types, for example --include=\"*.txt\" to download all prompts. Download the entire repository The following command illustrates how you can download the entire repository. Warning: running the following command will download ~380GB of data! rsync -av ultrasuite-rsync.inf.ed.ac.uk::ultrasuite .","title":"Download"},{"location":"download/#download","text":"The complete data from the UltraSuite repository is available via the rsync command. All flags from this tool should be valid when syncing the UltraSuite repository. Sample data from the UltraSuite repository is available via Edinburgh DataShare .","title":"Download"},{"location":"download/#download-sample-data","text":"Download of a very small subset of the UltraSuite repository can be achieve via Edinburgh DataShare . The downloadable datasets on DataShare contain a small sample from each of the three datasets in the UltraSuite repository, as well as their accompanying documentation. This is convenient for users who are unable or unwilling to download the repository using rsync or users that want a quick and easy way to look at the data.","title":"Download sample data"},{"location":"download/#windows-users","text":"rsync does not come pre-installed in Windows machines. If you are a Windows user, we have prepared instructions on how to setup rsync on your computer: Installing rsync on Windows Before going through those instructions, feel free to explore the sample data from Edinburgh DataShare and determine if the UltraSuite repository fit your needs. Additionally, as a Windows user, you might be interested in using Articulate Assistant Advanced (AAA) for your work. After downloading, the Ultrasuite data can be imported into the AAA software (revision 219.x or higher) one speaker at a time using the \u201cFile|Import|ultrasuite\u2026\u201d dialogue. First create a new project or open an existing project. The speaker will be created as a client in that project and all the selected files will be imported.","title":"Windows users"},{"location":"download/#using-rsync","text":"rsync is a command-line tool that allows efficient and fast transfer and synchronisation of files across systems. This is usually pre-install on Linux or mac machines. These instructions assume you have the tool installed on your system, but do not assume a lot of experience with its usage. If you are familiar with rsync , you can skip ahead and merely glance at the typical commands. All flags commonly used with the tool should be applicable to this scenario. See the man page for further details. If you are not familiar with rsync , you should be aware that, if you have a version of the data on your disk, rsync will synchronise your local data with the latest version of UltraSuite. If there are changes to the UltraSuite repository, your data will be overwritten with those latest changes. A useful flag to use with the tool is --dry-run , which will simulate the synchronisation process. Because the repository is quite large ( 380 GB ), please make sure you have enough space on your disk. Alternatively, go through these instructions and download only a subset of the data for which you have space for.","title":"Using rsync"},{"location":"download/#setting-up","text":"To get started, please create a directory on a hard drive where a large amount of space is available. Then, change your current directory onto that newly created one. On Linux or macOS, this can be achieved by running: mkdir UltraSuite cd UltraSuite","title":"Setting up"},{"location":"download/#download-an-utterance","text":"Now that we have a place to store the data, we will download some sample data from the repository. This might be useful if you wish to get a small example to get familiar with the format of the data or test some code. We first download a single utterance. This is a set of four files: a waveform (.wav), a text file with the prompt text and recording date time (.txt), the raw ultrasound data (ult), and a text file with parameters for the ultrasound data (.param). Running the following command should sync the first utterance from speaker 01M in the Ultrax Typically Developing (UXTD) data set. rsync -av ultrasuite-rsync.inf.ed.ac.uk::ultrasuite/core-uxtd/core/01M/001* . The sample we've just downloaded should give you an idea of the type of data available in UltraSuite. Speaker 01M in the UXTD set has 121 utterances. You can download different samples by changing 001* to the corresponding utterance. If you'd like, you can get all of the data for speaker 01M. This corresponds to 1.9GB ! To do so, please run the following command rsync -av ultrasuite-rsync.inf.ed.ac.uk::ultrasuite/core-uxtd/core/01M .","title":"Download an utterance"},{"location":"download/#download-a-data-set","text":"You can download an entire data set by syncing its corresponding directory. For example, for the UXTD data set (82GB), you can run the following command: rsync -av ultrasuite-rsync.inf.ed.ac.uk::ultrasuite/core-uxtd . The same command can be used to download /core-uxssd and /core-upx . Please be aware that each dataset can be quite large: Dataset Size /core-uxtd 82GB /core-uxssd 110GB /core-upx 191GB The structure of each data set is identical and it follows the following pattern: ./core-uxtd /doc /core /01M /02M ... Please see the list of speakers of each of the datasets, if you are interested in downloading data from single speakers. Each speaker directory will contain four data types: waveforms ( .wav ), prompts ( .txt ), ultrasound data ( .ult ), and ultrasound parameters ( .param ) . To simulate the process, you can run rsync with the flag --dry-run and get an estimate for the amount of data.","title":"Download a data set"},{"location":"download/#download-label-data","text":"To download manual and automatically-generated labels for all three datasets (~80MB), please run: rsync -av ultrasuite-rsync.inf.ed.ac.uk::ultrasuite/labels-uxtd-uxssd-upx . You can get labels for each of the datasets by appending /uxtd , /uxssd , or /upx to the command above. There is also a documentation directory for the labels called /doc . Previous versions of label data are archived in the top-level directory archive . To download an earlier version of label data, please run something like: rsync -av ultrasuite-rsync.inf.ed.ac.uk::ultrasuite/archive/labels-uxtd-uxssd-upx-1.0.zip . The command above will download version 1.0 of labels. Replace the version number as appropriate.","title":"Download label data"},{"location":"download/#download-selected-data-types","text":"Using rsync , it is possible to download only specific data types by specifying the desired extension. For example, you can download waveforms ( .wav ), prompts ( .txt ), ultrasound data ( .ult ), or ultrasound parameters ( .param ) separately. To achieve this, the rsync command would look like: rsync -av --include=\"*/\" --include=\"*.wav\" --exclude=\"*\" ultrasuite-rsync.inf.ed.ac.uk::ultrasuite/core-uxtd . This command downloads all waveforms from the UXTD dataset. Note that the --include flags works together with the --exclude flag. It is used to include files that would otherwise be excluded. The way to interpret this command is that we first exclude all files with --exclude=\"*\" . Then we include all directories with --include=\"*/\" and all files ending with the desired extension with --include=\"*.wav\" . Additional include flags may specify other data types, for example --include=\"*.txt\" to download all prompts.","title":"Download selected data types"},{"location":"download/#download-the-entire-repository","text":"The following command illustrates how you can download the entire repository. Warning: running the following command will download ~380GB of data! rsync -av ultrasuite-rsync.inf.ed.ac.uk::ultrasuite .","title":"Download the entire repository"},{"location":"download_win/","text":"Download The complete data from the UltraSuite repository is available via the rsync command. This page describes how to install rsync for Windows users. A very small subset of the data from the UltraSuite repository is also available via Edinburgh DataShare . Installing rsync on Windows Go to https://www.cygwin.com/install.html Download and run setup-x86_64.exe for Windows 64-bit or setup-x86.exe for Windows 32-bit. Click through all the defaults (pick any mirror site) until the \"Select packages\" dialogue. Select \"Full\" and in the search type \"rsync\". Double click on each \"skip\" in the \"New\" column to tell the installer to load these packages. Check \u201ccreate icon on desktop\u201d and \u201cadd icon to start menu\u201d and finish the install. Run Cygwin using icon on desktop or in start menu. Use cd to change directory and dir to show the contents of a directory. Change directory to somewhere (perhaps an external hard drive) with enough space to download the Ultrasuite data. Follow the instructions for downloading ultrasuite using rsync. Note : if you encounter problems try running Cygwin by right-clicking and selecting \u201crun as administrator\u201d. Acknowledgements: Many thanks to Alan Wrench from Articulate Instruments for preparing these instructions.","title":"Download"},{"location":"download_win/#download","text":"The complete data from the UltraSuite repository is available via the rsync command. This page describes how to install rsync for Windows users. A very small subset of the data from the UltraSuite repository is also available via Edinburgh DataShare .","title":"Download"},{"location":"download_win/#installing-rsync-on-windows","text":"Go to https://www.cygwin.com/install.html Download and run setup-x86_64.exe for Windows 64-bit or setup-x86.exe for Windows 32-bit. Click through all the defaults (pick any mirror site) until the \"Select packages\" dialogue. Select \"Full\" and in the search type \"rsync\". Double click on each \"skip\" in the \"New\" column to tell the installer to load these packages. Check \u201ccreate icon on desktop\u201d and \u201cadd icon to start menu\u201d and finish the install. Run Cygwin using icon on desktop or in start menu. Use cd to change directory and dir to show the contents of a directory. Change directory to somewhere (perhaps an external hard drive) with enough space to download the Ultrasuite data. Follow the instructions for downloading ultrasuite using rsync. Note : if you encounter problems try running Cygwin by right-clicking and selecting \u201crun as administrator\u201d.","title":"Installing rsync on Windows"},{"location":"download_win/#acknowledgements","text":"Many thanks to Alan Wrench from Articulate Instruments for preparing these instructions.","title":"Acknowledgements:"},{"location":"faq/","text":"Frequently Asked Questions Why is there a difference in the numbers reported here and those in the paper? Why is there silence in some waveforms? I found an issue in one or more utterances. How can I report it? What is this data repository being used for? How can I cite UltraSuite? Why is there a difference in the numbers reported here and those in the paper? Some of the numbers describing the repository in the paper, such as number of utterances or number of speech hours, were estimated at the time of the submission. We are actively working on improving UltraSuite, which means these number might change. For example, estimates for speech hours (child or therapist speech and silence) may change due to different speaker labelling methods or the removal of identifiable information. The number of utterances may change if we find unusable utterances (poor recording or corrupted data). To keep track of these changes, please be aware of version numbers in the datasets and their respective labels. Why is there silence in some waveforms? Some utterances contain regions of artificial silence. These correspond to regions originally containing identifiable information (such as children's names) and speech from additional speakers (such as parent speech). To remove such data, we have replaced them with silence in the waveform. I found an issue in one or more utterances. How can I report it? Please see Contributing . What is this data repository being used for? You can find out the various applications of this data by having a look at papers that cite Ultrasuite . If you are using the repository or know of any applications not listed in the link above, please get in touch with us. We would be happy to hear about it! How can I cite UltraSuite? For the current release of UltraSuite, if using data or code, please provide appropriate web links and cite the following paper: Eshky, A., Ribeiro, M. S., Cleland, J., Richmond, K., Roxburgh, Z., Scobbie, J., & Wrench, A. (2018) Ultrasuite: A repository of ultrasound and acoustic data from child speech therapy sessions . Proceedings of INTERSPEECH. Hyderabad, India. If you are using automatically-generated labels, you may also like to cite the following paper: Ribeiro, M. S., Eshky, A., Richmond, K. & Renals, S., (2019). Ultrasound tongue imaging for diarization and alignment of child speech therapy sessions . Proceedings of INTERSPEECH. Graz, Austria.","title":"Frequently Asked Questions"},{"location":"faq/#frequently-asked-questions","text":"Why is there a difference in the numbers reported here and those in the paper? Why is there silence in some waveforms? I found an issue in one or more utterances. How can I report it? What is this data repository being used for? How can I cite UltraSuite?","title":"Frequently Asked Questions"},{"location":"faq/#why-is-there-a-difference-in-the-numbers-reported-here-and-those-in-the-paper","text":"Some of the numbers describing the repository in the paper, such as number of utterances or number of speech hours, were estimated at the time of the submission. We are actively working on improving UltraSuite, which means these number might change. For example, estimates for speech hours (child or therapist speech and silence) may change due to different speaker labelling methods or the removal of identifiable information. The number of utterances may change if we find unusable utterances (poor recording or corrupted data). To keep track of these changes, please be aware of version numbers in the datasets and their respective labels.","title":"Why is there a difference in the numbers reported here and those in the paper?"},{"location":"faq/#why-is-there-silence-in-some-waveforms","text":"Some utterances contain regions of artificial silence. These correspond to regions originally containing identifiable information (such as children's names) and speech from additional speakers (such as parent speech). To remove such data, we have replaced them with silence in the waveform.","title":"Why is there silence in some waveforms?"},{"location":"faq/#i-found-an-issue-in-one-or-more-utterances-how-can-i-report-it","text":"Please see Contributing .","title":"I found an issue in one or more utterances. How can I report it?"},{"location":"faq/#what-is-this-data-repository-being-used-for","text":"You can find out the various applications of this data by having a look at papers that cite Ultrasuite . If you are using the repository or know of any applications not listed in the link above, please get in touch with us. We would be happy to hear about it!","title":"What is this data repository being used for?"},{"location":"faq/#how-can-i-cite-ultrasuite","text":"For the current release of UltraSuite, if using data or code, please provide appropriate web links and cite the following paper: Eshky, A., Ribeiro, M. S., Cleland, J., Richmond, K., Roxburgh, Z., Scobbie, J., & Wrench, A. (2018) Ultrasuite: A repository of ultrasound and acoustic data from child speech therapy sessions . Proceedings of INTERSPEECH. Hyderabad, India. If you are using automatically-generated labels, you may also like to cite the following paper: Ribeiro, M. S., Eshky, A., Richmond, K. & Renals, S., (2019). Ultrasound tongue imaging for diarization and alignment of child speech therapy sessions . Proceedings of INTERSPEECH. Graz, Austria.","title":"How can I cite UltraSuite?"},{"location":"data/cleft/","text":"The Cleft Palate Dataset A dataset of ultrasound and audio recorded with children with cleft lip and palate Speakers We recorded data with 39 English-speaking children, but only 29 gave consent to share their data. These are 11 female speakers and 18 male, aged 7-11 years. SPEAKER-ID GENDER AGE-Y AGE-M AGE CLEFT-TYPE OTHER-MEDICAL 01M M 10 5 10.42 BCLP no 03F F 5 1 5.83 UCLP no 05M M 10 3 10.75 UCLP no 06M M 9 8 9.75 UCLP no 07M M 9 2 9.75 UCLP no 09M M 9 8 9.33 UCLP yes 11M M 4 5 4.42 CP no 12F F 5 1 5.42 CP yes 14F F 5 0 5.33 BCLP no 15F F 4 9 4.75 UCLP yes 16M M 9 7 9.33 UCLP no 17F F 4 4 4.42 BCLP no 18F F 5 1 5.25 UCLP no 19F F 3 9 3.58 CP yes 20F F 7 5 7.75 CP no 21M M 9 1 9.5 CP no 24M M 6 5 6.33 CP yes 25M M 4 1 4.33 CP no 26M M 4 4 4.67 BCLP no 28F F 8 9 8.58 BCLP no 30F F 7 7 7.42 CP no 31F F 5 4 5.42 CP no 32M M 5 8 5.5 UCLP no 33M M 6 4 6.42 UCLP yes 34M M 5 3 5.25 CP yes 35M M 3 7 3.42 UCLP no 36M M 5 0 5.58 BCLP no 37M M 7 0 7.58 BCLP no 39M M 7 0 7 CP yes Cleft palate types Data type Description CP cleft palate only UCLP unilateral cleft lip and palate affecting one side of the lipand palate BLP bilateral cleft lip and palate affecting both sides Sessions Each child recorded an \"Assessment\" session, and two chilren recorded a \"Therapy\" session. Data Types Core data types Data type Description wav speech waveform ult raw ultrasound data param ultrasound parameters txt prompt text with date/time of utterance recording Additional data Data type Description slt_labels manual annotation from SLT, when available. See [2] for details probe_direction_labels a label for each utterance indicating whether the probe was in a coronal position (cor) or midgittal right or let (sag_right, sag_left) SLT Labels are available in Praat's TextGrid format, and probe_direction_labels is a csv file. File IDs Individual recordings are indexed for each session according to their recording times. See the prompt text file for recording date/time. Each file ID also includes a prompt type identifier. See Data for details. References [1] Eshky, A.,Cleland, J., Ribeiro, M. S., Renals, S. Automatic audiovisual synchronisation for ultrasound tongue imaging . Under revision. [2] Cleland, J., Lloyd, S., Campbell, L., Crampin, L., Palo, J.-P., Sugden,E., Wrench, A., & Zharkova, N. (2020). The impact of real-time ar-ticulatory information on phonetic transcription: ultrasound-aidedtranscription in cleft lip and palate speech . Folia Phoniatrica etLogopaedica, 72, 120\u2013130.","title":"The Cleft Palate Dataset"},{"location":"data/cleft/#the-cleft-palate-dataset","text":"A dataset of ultrasound and audio recorded with children with cleft lip and palate","title":"The Cleft Palate Dataset"},{"location":"data/cleft/#speakers","text":"We recorded data with 39 English-speaking children, but only 29 gave consent to share their data. These are 11 female speakers and 18 male, aged 7-11 years. SPEAKER-ID GENDER AGE-Y AGE-M AGE CLEFT-TYPE OTHER-MEDICAL 01M M 10 5 10.42 BCLP no 03F F 5 1 5.83 UCLP no 05M M 10 3 10.75 UCLP no 06M M 9 8 9.75 UCLP no 07M M 9 2 9.75 UCLP no 09M M 9 8 9.33 UCLP yes 11M M 4 5 4.42 CP no 12F F 5 1 5.42 CP yes 14F F 5 0 5.33 BCLP no 15F F 4 9 4.75 UCLP yes 16M M 9 7 9.33 UCLP no 17F F 4 4 4.42 BCLP no 18F F 5 1 5.25 UCLP no 19F F 3 9 3.58 CP yes 20F F 7 5 7.75 CP no 21M M 9 1 9.5 CP no 24M M 6 5 6.33 CP yes 25M M 4 1 4.33 CP no 26M M 4 4 4.67 BCLP no 28F F 8 9 8.58 BCLP no 30F F 7 7 7.42 CP no 31F F 5 4 5.42 CP no 32M M 5 8 5.5 UCLP no 33M M 6 4 6.42 UCLP yes 34M M 5 3 5.25 CP yes 35M M 3 7 3.42 UCLP no 36M M 5 0 5.58 BCLP no 37M M 7 0 7.58 BCLP no 39M M 7 0 7 CP yes","title":"Speakers"},{"location":"data/cleft/#cleft-palate-types","text":"Data type Description CP cleft palate only UCLP unilateral cleft lip and palate affecting one side of the lipand palate BLP bilateral cleft lip and palate affecting both sides","title":"Cleft palate types"},{"location":"data/cleft/#sessions","text":"Each child recorded an \"Assessment\" session, and two chilren recorded a \"Therapy\" session.","title":"Sessions"},{"location":"data/cleft/#data-types","text":"Core data types Data type Description wav speech waveform ult raw ultrasound data param ultrasound parameters txt prompt text with date/time of utterance recording Additional data Data type Description slt_labels manual annotation from SLT, when available. See [2] for details probe_direction_labels a label for each utterance indicating whether the probe was in a coronal position (cor) or midgittal right or let (sag_right, sag_left) SLT Labels are available in Praat's TextGrid format, and probe_direction_labels is a csv file.","title":"Data Types"},{"location":"data/cleft/#file-ids","text":"Individual recordings are indexed for each session according to their recording times. See the prompt text file for recording date/time. Each file ID also includes a prompt type identifier. See Data for details.","title":"File IDs"},{"location":"data/cleft/#references","text":"[1] Eshky, A.,Cleland, J., Ribeiro, M. S., Renals, S. Automatic audiovisual synchronisation for ultrasound tongue imaging . Under revision. [2] Cleland, J., Lloyd, S., Campbell, L., Crampin, L., Palo, J.-P., Sugden,E., Wrench, A., & Zharkova, N. (2020). The impact of real-time ar-ticulatory information on phonetic transcription: ultrasound-aidedtranscription in cleft lip and palate speech . Folia Phoniatrica etLogopaedica, 72, 120\u2013130.","title":"References"},{"location":"data/data/","text":"Data Description Prompt Types There are six different prompt types, encoding the task with was requested of the child. Please see section 3.1 of Eshky et al (2018) for further details. Type ID UXTD UXSSD UPX Words A 962 (26) 2708 (291) 3838 (455) Non-words B 607 (27) 495 (59) 560 (60) Sentence C 0 (0) 445 (35) 1020 (128) Articulatory D 2934 (45) 132 (17) 211 (21) Non-speech E 116 (2) 9 (1) 302 (1) Other F 0 (0) 56 (12) 61 (1) Total 4619 (100) 3845 (415) 5992 (682) Speech Hours The following hours of speech and silence are rounded to two decimal places and are estimated using the speaker labels that accompany each dataset. UXTD UXSSD UPX Child speech 2.24 (28.39%) 3.66 (34.45%) 7.27 (38.70%) SLT speech 1.24 (15.74%) 1.81 (16.99%) 1.92 (10.23%) Total speech 3.47 (44.12%) 5.47 (51.43%) 9.19 (48.93%) Initial silence 1.41 (17.96%) 0.91 (8.55%) 0.78 (4.17%) Medial silence 1.99 (25.30%) 3.48 (32.69%) 7.11 (37.83%) Final silence 0.99 (12.61%) 0.78 (7.32%) 1.70 (9.07%) Total silence 4.40 (55.88%) 5.16 (48.57%) 9.59 (51.07%) Total Audio 7.87 10.63 18.78","title":"Data Description"},{"location":"data/data/#data-description","text":"","title":"Data Description"},{"location":"data/data/#prompt-types","text":"There are six different prompt types, encoding the task with was requested of the child. Please see section 3.1 of Eshky et al (2018) for further details. Type ID UXTD UXSSD UPX Words A 962 (26) 2708 (291) 3838 (455) Non-words B 607 (27) 495 (59) 560 (60) Sentence C 0 (0) 445 (35) 1020 (128) Articulatory D 2934 (45) 132 (17) 211 (21) Non-speech E 116 (2) 9 (1) 302 (1) Other F 0 (0) 56 (12) 61 (1) Total 4619 (100) 3845 (415) 5992 (682)","title":"Prompt Types"},{"location":"data/data/#speech-hours","text":"The following hours of speech and silence are rounded to two decimal places and are estimated using the speaker labels that accompany each dataset. UXTD UXSSD UPX Child speech 2.24 (28.39%) 3.66 (34.45%) 7.27 (38.70%) SLT speech 1.24 (15.74%) 1.81 (16.99%) 1.92 (10.23%) Total speech 3.47 (44.12%) 5.47 (51.43%) 9.19 (48.93%) Initial silence 1.41 (17.96%) 0.91 (8.55%) 0.78 (4.17%) Medial silence 1.99 (25.30%) 3.48 (32.69%) 7.11 (37.83%) Final silence 0.99 (12.61%) 0.78 (7.32%) 1.70 (9.07%) Total silence 4.40 (55.88%) 5.16 (48.57%) 9.59 (51.07%) Total Audio 7.87 10.63 18.78","title":"Speech Hours"},{"location":"data/tal_corpus/","text":"The Tongue and Lips Corpus A multi-speaker corpus of ultrasound images of the tongue and video images of the lips The Tongue and Lips (TaL) corpus is a multi-speaker corpus of ultrasound images of the tongue and video images of lips. This corpus contains synchronised imaging data of extraoral (lips) and intraoral (tongue) articulators from 82 native speakers of English. For more information, please read the TaL corpus paper here! Datasets The TaL corpus consists of two datasets: TaL1 is a single-speaker dataset containing data of one professional voice talent, a male native speaker of English, over six recording sessions. TaL80 is a multi-speaker dataset contains recording sessions of 81 native speakers of English without voice talent experience. Each speaker was recording over a single recording session. Speaker and session identifiers In the TaL80 dataset, speaker identifiers denote speaker number, gender (m/f), and country of origin. Country identifiers are: (e)ngland, (s)cotland, (i)reland, (n)orthern-ireland, (o)ther. Examples: 01fi, 02fe, 03mn, 04me, ... The TaL1 dataset only has 1 speaker, so there are no speaker identifiers. Instead, we have recording sessions, which are simply called day1 , day2 , day3 , ... File identifiers For each speaker (TaL80) or session (TaL1), utterances are indexed according to their recording times. See the prompt text file for recording date/time. Each file ID also includes a tag indicating the prompt type. Prompt Tag Description swa swallow cal calibration aud audible read speech sil silent speech whi whispered speech (TaL1 only) spo spontaneous speech utterance (unprompted speech) xaud shared audible read speech utterances xsil shared silent speech utterances xwhi shared whispered speech utterance (TaL1 only) Calibration prompts ( cal ) and swallows ( swa ) were read at the beginning and end of each recording session and before and after a short break. The tag x denotes prompts that were shared across speakers (TaL80) or recording sessions (TaL1). Examples: 001_swa, 002_cal, 004_xaud, 028_spo, 029_xsil, 038_sil, ... Data types Each utterance consists of five core data types, which can be identified by their file extension. Core data types Data type Extension Description prompt .txt text file with prompt and datetime of recording waveform .wav speech waveform synchronisation .sync audio synchronisation signal (waveform) ultrasound .ult, .param raw ultrasound data (.ult) and ultrasound parameters (.param) video .mp4 video images of the lips (synchronised to waveform) Example. The second utterance recorded by speaker 01fi is a calibration utterance with the identifier 002_cal . The five core data types for this utterance are the files: 002_cal.txt, 002_cal.wav, 002_cal.sync, 002_cal.ult, 002_cal.param, 002_cal.mp3. Additional data Because spontaneous speech utterances can be long in duration (up to 60 seconds), we manually annotated the boundaries of shorter time segments (typically 5-10 seconds). This annotation is available as a CSV file with start and end time in seconds of the short segments nd their respective transcription. This file is identified by the extension .lab . Structure TaL1 and TaL80 follow a similar structure, but they are independent datasets. For this reason, shared prompts are only marked within datasets (across speakers for TaL80 and sessions for TaL1). There is an overlap in the recorded prompts in the two datasets. Most prompts read in TaL1 were recorded by the first speakers in TaL80, but a small subset was read by all speakers. Users should be aware of this if using both datasets, particularly when designing training and test splits. Directory structure for TaL1: /TaL1 /samples /core /video /core /day2 /day3 ... /doc Directory structure for TaL80: /TaL80 /samples /core /video /core /01fi /02fe /03mn ... /doc The samples directory contains a subset of the larger dataset (2 samples per speaker/session). If you wish to have a quick look at the TaL corpus, you can download this directory first and browse some examples. The directory samples/core provides a subset of the core data types and the directory samples/video provides video samples generated with the tal-tools visualiser . The doc directory contains the documentation for the data, as well as some additional documents, such as version number and anonymised participant information. The core directory contains the core data for the dataset. Video samples In samples/video , there are a few video examples generated with the tal-tools visualiser . These sample videos are also available online: TaL1 video samples . TaL80 video samples Download The datasets are quite large, so please make sure that you have enough disk space before attempting to download. Dataset Size /TaL1/core 49GB /TaL80/core 498GB If you prefer to browse some samples before downloading the full data, you can download the samples directories. Dataset Size /TaL1/samples 2.1GB /TaL80/samples 8.2GB To download the TaL corpus, please check the download instructions for the Ultrasuite repository . The instrutions are applicable to the TaL corpus, in case you prefer to download part of the data (an utterance, a specific data type, etc). However, note that we replace ultrasuite-rsync.inf.ed.ac.uk::ultrasuite with ultrasuite-rsync.inf.ed.ac.uk::tal-corpus . Warning : the commands below will download 49GB and 498GB of data, respectively! Please make sure you have enough disk space. Check the download instructions for the Ultrasuite repository to download subsets of the data. To download the TaL1 dataset, you can run: rsync -av ultrasuite-rsync.inf.ed.ac.uk::tal-corpus/TaL1 . Similarly, to download the TaL80 dataset, you can run: rsync -av ultrasuite-rsync.inf.ed.ac.uk::tal-corpus/TaL80 . Using the data The video data released with the TaL corpus does not embed the audio. If you wish to see the video with the corresponding waveform, you can use ffmpeg with a command such as: ffmpeg -i input.mp4 -i input.wav -c:v copy -map 0:v:0 -map 1:a:0 -c:a ac3 -b:a 192k output.mp4 If you wish to visualise the ultrasound, with or without audio, you can use Ultrasuite tools . For more complex visualisations including video, ultrasound, and spectrogram/waveform, please have a tool at the TaL corpus visualiser . If you're just interested in general input/output, one or more of these functions should provide some useful examples. Additional Notes TaL1 Notes The video synchronisation failed during the first recording session of the TaL1 data. The problem can be seen in the synchronisation signal, which merged the video and ultrasound signals. We opted to release this session, as it might still be useful for some applications that do not depend on video and audio synchronisation. This session is named day1_no_vid_sync . TaL80 Notes Please see the participant notes in TaL80/doc for anonymised detailed notes on all participants. We describe here two cases where image quality was not as good as we hoped. Speaker 17ms has a large amount of facial hair, which hides a large portion of the lips in the video. The ultrasound images of the tongue appear reasonable. Speaker 60ms has a large amount of facial hair under the chin, which created some problems for the ultrasound probe. The video, however, appears reasonable. Acknowledgements Supported by the Carnegie Trust for the Universities of Scotland (Research Incentive Grant number 008585) and the EPSRC Healthcare Partnerships grant number EP/P02338X/1 (Ultrax2020). We thank the participants of this corpus for providing the consent that allows this data to be freely available to the research community. References If using data or code from the TaL corpus, please provide appropriate web links and cite the following paper: Ribeiro, M. S., Sanger, J., Zhang, J.-X., Eshky, A., Wrench, A., Richmond, K.,& Renals, S. (2021). TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging, audio, and lip videos. Proceedings of the IEEE Workshop on Spoken Language Technology (SLT). Shenzhen, China. [ paper ]","title":"TaL Corpus"},{"location":"data/tal_corpus/#the-tongue-and-lips-corpus","text":"A multi-speaker corpus of ultrasound images of the tongue and video images of the lips The Tongue and Lips (TaL) corpus is a multi-speaker corpus of ultrasound images of the tongue and video images of lips. This corpus contains synchronised imaging data of extraoral (lips) and intraoral (tongue) articulators from 82 native speakers of English. For more information, please read the TaL corpus paper here!","title":"The Tongue and Lips Corpus"},{"location":"data/tal_corpus/#datasets","text":"The TaL corpus consists of two datasets: TaL1 is a single-speaker dataset containing data of one professional voice talent, a male native speaker of English, over six recording sessions. TaL80 is a multi-speaker dataset contains recording sessions of 81 native speakers of English without voice talent experience. Each speaker was recording over a single recording session.","title":"Datasets"},{"location":"data/tal_corpus/#speaker-and-session-identifiers","text":"In the TaL80 dataset, speaker identifiers denote speaker number, gender (m/f), and country of origin. Country identifiers are: (e)ngland, (s)cotland, (i)reland, (n)orthern-ireland, (o)ther. Examples: 01fi, 02fe, 03mn, 04me, ... The TaL1 dataset only has 1 speaker, so there are no speaker identifiers. Instead, we have recording sessions, which are simply called day1 , day2 , day3 , ...","title":"Speaker and session identifiers"},{"location":"data/tal_corpus/#file-identifiers","text":"For each speaker (TaL80) or session (TaL1), utterances are indexed according to their recording times. See the prompt text file for recording date/time. Each file ID also includes a tag indicating the prompt type. Prompt Tag Description swa swallow cal calibration aud audible read speech sil silent speech whi whispered speech (TaL1 only) spo spontaneous speech utterance (unprompted speech) xaud shared audible read speech utterances xsil shared silent speech utterances xwhi shared whispered speech utterance (TaL1 only) Calibration prompts ( cal ) and swallows ( swa ) were read at the beginning and end of each recording session and before and after a short break. The tag x denotes prompts that were shared across speakers (TaL80) or recording sessions (TaL1). Examples: 001_swa, 002_cal, 004_xaud, 028_spo, 029_xsil, 038_sil, ...","title":"File identifiers"},{"location":"data/tal_corpus/#data-types","text":"Each utterance consists of five core data types, which can be identified by their file extension. Core data types Data type Extension Description prompt .txt text file with prompt and datetime of recording waveform .wav speech waveform synchronisation .sync audio synchronisation signal (waveform) ultrasound .ult, .param raw ultrasound data (.ult) and ultrasound parameters (.param) video .mp4 video images of the lips (synchronised to waveform) Example. The second utterance recorded by speaker 01fi is a calibration utterance with the identifier 002_cal . The five core data types for this utterance are the files: 002_cal.txt, 002_cal.wav, 002_cal.sync, 002_cal.ult, 002_cal.param, 002_cal.mp3. Additional data Because spontaneous speech utterances can be long in duration (up to 60 seconds), we manually annotated the boundaries of shorter time segments (typically 5-10 seconds). This annotation is available as a CSV file with start and end time in seconds of the short segments nd their respective transcription. This file is identified by the extension .lab .","title":"Data types"},{"location":"data/tal_corpus/#structure","text":"TaL1 and TaL80 follow a similar structure, but they are independent datasets. For this reason, shared prompts are only marked within datasets (across speakers for TaL80 and sessions for TaL1). There is an overlap in the recorded prompts in the two datasets. Most prompts read in TaL1 were recorded by the first speakers in TaL80, but a small subset was read by all speakers. Users should be aware of this if using both datasets, particularly when designing training and test splits. Directory structure for TaL1: /TaL1 /samples /core /video /core /day2 /day3 ... /doc Directory structure for TaL80: /TaL80 /samples /core /video /core /01fi /02fe /03mn ... /doc The samples directory contains a subset of the larger dataset (2 samples per speaker/session). If you wish to have a quick look at the TaL corpus, you can download this directory first and browse some examples. The directory samples/core provides a subset of the core data types and the directory samples/video provides video samples generated with the tal-tools visualiser . The doc directory contains the documentation for the data, as well as some additional documents, such as version number and anonymised participant information. The core directory contains the core data for the dataset.","title":"Structure"},{"location":"data/tal_corpus/#video-samples","text":"In samples/video , there are a few video examples generated with the tal-tools visualiser . These sample videos are also available online: TaL1 video samples . TaL80 video samples","title":"Video samples"},{"location":"data/tal_corpus/#download","text":"The datasets are quite large, so please make sure that you have enough disk space before attempting to download. Dataset Size /TaL1/core 49GB /TaL80/core 498GB If you prefer to browse some samples before downloading the full data, you can download the samples directories. Dataset Size /TaL1/samples 2.1GB /TaL80/samples 8.2GB To download the TaL corpus, please check the download instructions for the Ultrasuite repository . The instrutions are applicable to the TaL corpus, in case you prefer to download part of the data (an utterance, a specific data type, etc). However, note that we replace ultrasuite-rsync.inf.ed.ac.uk::ultrasuite with ultrasuite-rsync.inf.ed.ac.uk::tal-corpus . Warning : the commands below will download 49GB and 498GB of data, respectively! Please make sure you have enough disk space. Check the download instructions for the Ultrasuite repository to download subsets of the data. To download the TaL1 dataset, you can run: rsync -av ultrasuite-rsync.inf.ed.ac.uk::tal-corpus/TaL1 . Similarly, to download the TaL80 dataset, you can run: rsync -av ultrasuite-rsync.inf.ed.ac.uk::tal-corpus/TaL80 .","title":"Download"},{"location":"data/tal_corpus/#using-the-data","text":"The video data released with the TaL corpus does not embed the audio. If you wish to see the video with the corresponding waveform, you can use ffmpeg with a command such as: ffmpeg -i input.mp4 -i input.wav -c:v copy -map 0:v:0 -map 1:a:0 -c:a ac3 -b:a 192k output.mp4 If you wish to visualise the ultrasound, with or without audio, you can use Ultrasuite tools . For more complex visualisations including video, ultrasound, and spectrogram/waveform, please have a tool at the TaL corpus visualiser . If you're just interested in general input/output, one or more of these functions should provide some useful examples.","title":"Using the data"},{"location":"data/tal_corpus/#additional-notes","text":"TaL1 Notes The video synchronisation failed during the first recording session of the TaL1 data. The problem can be seen in the synchronisation signal, which merged the video and ultrasound signals. We opted to release this session, as it might still be useful for some applications that do not depend on video and audio synchronisation. This session is named day1_no_vid_sync . TaL80 Notes Please see the participant notes in TaL80/doc for anonymised detailed notes on all participants. We describe here two cases where image quality was not as good as we hoped. Speaker 17ms has a large amount of facial hair, which hides a large portion of the lips in the video. The ultrasound images of the tongue appear reasonable. Speaker 60ms has a large amount of facial hair under the chin, which created some problems for the ultrasound probe. The video, however, appears reasonable.","title":"Additional Notes"},{"location":"data/tal_corpus/#acknowledgements","text":"Supported by the Carnegie Trust for the Universities of Scotland (Research Incentive Grant number 008585) and the EPSRC Healthcare Partnerships grant number EP/P02338X/1 (Ultrax2020). We thank the participants of this corpus for providing the consent that allows this data to be freely available to the research community.","title":"Acknowledgements"},{"location":"data/tal_corpus/#references","text":"If using data or code from the TaL corpus, please provide appropriate web links and cite the following paper: Ribeiro, M. S., Sanger, J., Zhang, J.-X., Eshky, A., Wrench, A., Richmond, K.,& Renals, S. (2021). TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging, audio, and lip videos. Proceedings of the IEEE Workshop on Spoken Language Technology (SLT). Shenzhen, China. [ paper ]","title":"References"},{"location":"data/upx-spk/","text":"UPX - UltraPhonix A dataset of ultrasound and audio recordings from typically developing children Speaker list of UltraSuite's UPX dataset. SPEAKER-ID GENDER AGE-Y AGE-M AGE SSD SUBTYPE 01F Female 8 8 8.67 inconsistent phonological disorder 02F Female 7 8 7.67 phonological disorder 03F Female 10 11 10.92 childhood apraxia of speech 04M Male 7 2 7.17 phonological delay 05M Male 6 5 6.42 vowel disorder 06M Male 6 4 6.33 phonological delay 07M Male 8 11 8.92 childhood apraxia of speech 08M Male 10 2 10.17 childhood apraxia of speech 09M Male 7 9 7.75 phonological disorder 10M Male 13 4 13.33 childhood apraxia of speech 11M Male 6 7 6.58 phonological delay 12M Male 6 3 6.25 phonological delay 13M Male 7 4 7.33 inconsistent phonological disorder 14M Male 6 10 6.83 vowel disorder 15M Male 6 1 6.08 phonological delay 16M Male 7 7 7.58 articulation disorder 17M Male 13 2 13.17 phonological delay 18F Female 7 1 7.08 articulation disorder 19M Male 10 0 10.0 phonological delay 20M Male 9 2 9.17 articulation disorder","title":"UPX - UltraPhonix"},{"location":"data/upx-spk/#upx-ultraphonix","text":"","title":"UPX - UltraPhonix"},{"location":"data/upx-spk/#a-dataset-of-ultrasound-and-audio-recordings-from-typically-developing-children","text":"Speaker list of UltraSuite's UPX dataset. SPEAKER-ID GENDER AGE-Y AGE-M AGE SSD SUBTYPE 01F Female 8 8 8.67 inconsistent phonological disorder 02F Female 7 8 7.67 phonological disorder 03F Female 10 11 10.92 childhood apraxia of speech 04M Male 7 2 7.17 phonological delay 05M Male 6 5 6.42 vowel disorder 06M Male 6 4 6.33 phonological delay 07M Male 8 11 8.92 childhood apraxia of speech 08M Male 10 2 10.17 childhood apraxia of speech 09M Male 7 9 7.75 phonological disorder 10M Male 13 4 13.33 childhood apraxia of speech 11M Male 6 7 6.58 phonological delay 12M Male 6 3 6.25 phonological delay 13M Male 7 4 7.33 inconsistent phonological disorder 14M Male 6 10 6.83 vowel disorder 15M Male 6 1 6.08 phonological delay 16M Male 7 7 7.58 articulation disorder 17M Male 13 2 13.17 phonological delay 18F Female 7 1 7.08 articulation disorder 19M Male 10 0 10.0 phonological delay 20M Male 9 2 9.17 articulation disorder","title":"A dataset of ultrasound and audio recordings from typically developing children"},{"location":"data/upx/","text":"UltraPhonix A dataset of ultrasound and audio recordings from children with speech sound disorders Speakers The UltraPhonix dataset contains 20 speakers (16 male, 4 female), aged 6-13 years. For a list and additional details, see UPX Speakers . Sessions Session Description Suit Suitability session to determine if child needs speech therapy BL Baseline session before therapy (1-2 sessions) Mid Mid-point session, halfway through therapy Post Post-therapy session, immediately after therapy ended Maint Maintenance session, some time after therapy ended Therapy Therapy sessions Data Types Core data types Data type Description wav speech waveform ult raw ultrasound data param ultrasound parameters txt prompt text with date/time of utterance recording Additional data Data type Description slt_labels manual annotation from SLT, when available. See [2] for details speaker_labels speaker diarization identifying therapist (SLT) and child (CHILD) speech word_labels automatic word-level alignment phone_labels automatic phone-level alignment Labels are available in Praat's TextGrid format and HTK's lab format. Speaker, word, and phone labels were generated according to the methods described in [4]. File IDs Individual recordings are indexed for each session according to their recording times. See the prompt text file for recording date/time. Each file ID also includes a prompt type identifier. See Data for details. References [1] Eshky, A., Ribeiro, M. S., Cleland, J., Richmond, K., Roxburgh, Z., Scobbie, J., & Wrench, A. (2018) Ultrasuite: A repository of ultrasound and acoustic data from child speech therapy sessions . Proceedings of INTERSPEECH. Hyderabad, India. [2] Cleland, J., Scobbie, J. M., Heyde, C., Roxburgh, Z., & Wrench, A. A. (2017). Covert contrast and covert errors in persistent velar fronting . Clinical linguistics & phonetics, 31(1), 35-55. [3] Cleland, J., Scobbie, J. M., Roxburgh, Z., Heyde, C., & Wrench, A. A. (Under Revision). Enabling New Articulatory Gestures in Children with Persistent Speech Sound Disorders using Ultrasound Visual Biofeedback . Journal of Speech, Language, and Hearing Research. [4] Ribeiro, M. S., Eshky, A., Richmond, K., Renals, S., (2019). Ultrasound tongue imaging for diarization and alignment of child speech therapy sessions . Proceedings of INTERSPEECH. Graz, Austria.","title":"Ultraphonix"},{"location":"data/upx/#ultraphonix","text":"A dataset of ultrasound and audio recordings from children with speech sound disorders","title":"UltraPhonix"},{"location":"data/upx/#speakers","text":"The UltraPhonix dataset contains 20 speakers (16 male, 4 female), aged 6-13 years. For a list and additional details, see UPX Speakers .","title":"Speakers"},{"location":"data/upx/#sessions","text":"Session Description Suit Suitability session to determine if child needs speech therapy BL Baseline session before therapy (1-2 sessions) Mid Mid-point session, halfway through therapy Post Post-therapy session, immediately after therapy ended Maint Maintenance session, some time after therapy ended Therapy Therapy sessions","title":"Sessions"},{"location":"data/upx/#data-types","text":"Core data types Data type Description wav speech waveform ult raw ultrasound data param ultrasound parameters txt prompt text with date/time of utterance recording Additional data Data type Description slt_labels manual annotation from SLT, when available. See [2] for details speaker_labels speaker diarization identifying therapist (SLT) and child (CHILD) speech word_labels automatic word-level alignment phone_labels automatic phone-level alignment Labels are available in Praat's TextGrid format and HTK's lab format. Speaker, word, and phone labels were generated according to the methods described in [4].","title":"Data Types"},{"location":"data/upx/#file-ids","text":"Individual recordings are indexed for each session according to their recording times. See the prompt text file for recording date/time. Each file ID also includes a prompt type identifier. See Data for details.","title":"File IDs"},{"location":"data/upx/#references","text":"[1] Eshky, A., Ribeiro, M. S., Cleland, J., Richmond, K., Roxburgh, Z., Scobbie, J., & Wrench, A. (2018) Ultrasuite: A repository of ultrasound and acoustic data from child speech therapy sessions . Proceedings of INTERSPEECH. Hyderabad, India. [2] Cleland, J., Scobbie, J. M., Heyde, C., Roxburgh, Z., & Wrench, A. A. (2017). Covert contrast and covert errors in persistent velar fronting . Clinical linguistics & phonetics, 31(1), 35-55. [3] Cleland, J., Scobbie, J. M., Roxburgh, Z., Heyde, C., & Wrench, A. A. (Under Revision). Enabling New Articulatory Gestures in Children with Persistent Speech Sound Disorders using Ultrasound Visual Biofeedback . Journal of Speech, Language, and Hearing Research. [4] Ribeiro, M. S., Eshky, A., Richmond, K., Renals, S., (2019). Ultrasound tongue imaging for diarization and alignment of child speech therapy sessions . Proceedings of INTERSPEECH. Graz, Austria.","title":"References"},{"location":"data/uxssd/","text":"Ultrax Speech Sound Disorders A dataset of ultrasound and audio recordings from children with speech sound disorders Speakers The UXSSD dataset contains 8 speakers (2 female and 6 male), aged 5-10 years. The table below give further details for each speaker. Ages were taken in the first Assessment session and are indicated in years (AGE-Y) and months (AGE-M). SPEAKER-ID GENDER AGE-Y AGE-M AGE 01M M 6 0 6.0 02M M 10 1 10.08 03F F 8 7 8.58 04M M 8 11 8.92 05M M 6 5 6.42 06M M 5 11 5.92 07F F 7 6 7.5 08M M 7 7 7.58 Sessions Session Description BL Baseline session before therapy (1-2 sessions) Mid Mid-point session, halfway through therapy Post Post-therapy session, immediately after therapy ended Maint Maintenance session, some time after therapy ended Therapy Therapy sessions Data Types Core data types Data type Description wav speech waveform ult raw ultrasound data param ultrasound parameters txt prompt text with date/time of utterance recording Additional data Data type Description slt_labels manual annotation from SLT, when available. See [2] for details speaker_labels speaker diarization identifying therapist (SLT) and child (CHILD) speech word_labels automatic word-level alignment phone_labels automatic phone-level alignment reference_labels manually-revised labels (see below for details) Labels are available in Praat's TextGrid format and HTK's lab format. Speaker, word, and phone labels were generated according to the methods described in [3]. File IDs Individual recordings are indexed for each session according to their recording times. See the prompt text file for recording date/time. Each file ID also includes a prompt type identifier. See Data for details. Reference Labels Reference labels are given for a few utterances of the UXTD and UXSSD datasets. These have been manually revised at the speaker (60 utterances) and word level (199 utterances). The revision was done by a single annotator. Note that phone labels are also provided, but these are not entirely manually-revised . This set of annotation is force-aligned at the phone level, but constrained by the manually-revised word boundaries. Labels are available in Praat's TextGrid format ( TG ) and HTK's lab format ( lab ). The structure for the directory is as follows: /uxssd /phone_labels /lab /TG /word_labels /lab /TG /speaker_labels /lab /TG Additional Notes Speaker 05M was subjected to two rounds of therapy, with corresponding Assessment sessions. These are identified as *_round2 in the speaker directory. Therapy sessions for this speaker are indexed chronologically. References [1] Eshky, A., Ribeiro, M. S., Cleland, J., Richmond, K., Roxburgh, Z., Scobbie, J., & Wrench, A. (2018) Ultrasuite: A repository of ultrasound and acoustic data from child speech therapy sessions . Proceedings of INTERSPEECH. Hyderabad, India. [2] Cleland, J., Scobbie, J. M., & Wrench, A. A. (2015). Using ultrasound visual biofeedback to treat persistent primary speech sound disorders . Clinical linguistics & phonetics, 29(8-10), 575-597. [3] Ribeiro, M. S., Eshky, A., Richmond, K., Renals, S., (2019). Ultrasound tongue imaging for diarization and alignment of child speech therapy sessions . Proceedings of INTERSPEECH. Graz, Austria.","title":"Ultrax Speech Sound Disorders"},{"location":"data/uxssd/#ultrax-speech-sound-disorders","text":"A dataset of ultrasound and audio recordings from children with speech sound disorders","title":"Ultrax Speech Sound Disorders"},{"location":"data/uxssd/#speakers","text":"The UXSSD dataset contains 8 speakers (2 female and 6 male), aged 5-10 years. The table below give further details for each speaker. Ages were taken in the first Assessment session and are indicated in years (AGE-Y) and months (AGE-M). SPEAKER-ID GENDER AGE-Y AGE-M AGE 01M M 6 0 6.0 02M M 10 1 10.08 03F F 8 7 8.58 04M M 8 11 8.92 05M M 6 5 6.42 06M M 5 11 5.92 07F F 7 6 7.5 08M M 7 7 7.58 Sessions Session Description BL Baseline session before therapy (1-2 sessions) Mid Mid-point session, halfway through therapy Post Post-therapy session, immediately after therapy ended Maint Maintenance session, some time after therapy ended Therapy Therapy sessions","title":"Speakers"},{"location":"data/uxssd/#data-types","text":"Core data types Data type Description wav speech waveform ult raw ultrasound data param ultrasound parameters txt prompt text with date/time of utterance recording Additional data Data type Description slt_labels manual annotation from SLT, when available. See [2] for details speaker_labels speaker diarization identifying therapist (SLT) and child (CHILD) speech word_labels automatic word-level alignment phone_labels automatic phone-level alignment reference_labels manually-revised labels (see below for details) Labels are available in Praat's TextGrid format and HTK's lab format. Speaker, word, and phone labels were generated according to the methods described in [3].","title":"Data Types"},{"location":"data/uxssd/#file-ids","text":"Individual recordings are indexed for each session according to their recording times. See the prompt text file for recording date/time. Each file ID also includes a prompt type identifier. See Data for details.","title":"File IDs"},{"location":"data/uxssd/#reference-labels","text":"Reference labels are given for a few utterances of the UXTD and UXSSD datasets. These have been manually revised at the speaker (60 utterances) and word level (199 utterances). The revision was done by a single annotator. Note that phone labels are also provided, but these are not entirely manually-revised . This set of annotation is force-aligned at the phone level, but constrained by the manually-revised word boundaries. Labels are available in Praat's TextGrid format ( TG ) and HTK's lab format ( lab ). The structure for the directory is as follows: /uxssd /phone_labels /lab /TG /word_labels /lab /TG /speaker_labels /lab /TG","title":"Reference Labels"},{"location":"data/uxssd/#additional-notes","text":"Speaker 05M was subjected to two rounds of therapy, with corresponding Assessment sessions. These are identified as *_round2 in the speaker directory. Therapy sessions for this speaker are indexed chronologically.","title":"Additional Notes"},{"location":"data/uxssd/#references","text":"[1] Eshky, A., Ribeiro, M. S., Cleland, J., Richmond, K., Roxburgh, Z., Scobbie, J., & Wrench, A. (2018) Ultrasuite: A repository of ultrasound and acoustic data from child speech therapy sessions . Proceedings of INTERSPEECH. Hyderabad, India. [2] Cleland, J., Scobbie, J. M., & Wrench, A. A. (2015). Using ultrasound visual biofeedback to treat persistent primary speech sound disorders . Clinical linguistics & phonetics, 29(8-10), 575-597. [3] Ribeiro, M. S., Eshky, A., Richmond, K., Renals, S., (2019). Ultrasound tongue imaging for diarization and alignment of child speech therapy sessions . Proceedings of INTERSPEECH. Graz, Austria.","title":"References"},{"location":"data/uxtd-spk/","text":"UXTD - Ultrax Typically Developing Children A dataset of ultrasound and audio recordings from typically developing children Speaker list of UltraSuite's UXTD dataset. Speakers SPEAKER-ID GROUP GENDER AGE-Y AGE-M AGE RECORDING DATE SUBSET 01M 1 M 11 10 11.83 13/11/2011 TRAIN 02M 1 M 11 9 11.75 22/12/2011 TRAIN 03F 1 F 10 5 10.42 27/01/2011 TRAIN 04M 1 M 8 9 8.75 27/01/2011 TRAIN 05M 1 M 9 11 9.92 03/02/2012 TRAIN 06F 1 F 9 10 9.83 14/02/2012 TRAIN 07F 1 F 9 4 9.33 14/02/2012 TEST 08M 1 M 8 8 8.67 16/02/2012 TEST 09F 1 F 6 9 6.75 16/02/2012 TRAIN 10F 1 F 11 3 11.25 17/02/2012 TRAIN 11M 1 M 8 1 8.08 09/03/2012 TRAIN 12M 1 M 6 8 6.67 15/03/2012 TEST 13F 1 F 11 7 11.58 21/03/2012 TEST 14M 1 M 12 4 12.33 21/03/2012 TRAIN 15M 1 M 7 11 7.92 21/03/2012 TRAIN 16F 1 F 12 10 12.83 21/03/2012 TRAIN 17M 1 M 10 8 10.67 21/02/2012 DEV 18F 1 F 7 2 7.17 30/03/2012 TRAIN 19M 1 M 12 6 12.5 30/03/2012 TRAIN 20M 1 M 11 1 11.08 02/04/2012 TRAIN 21F 1 F 5 8 5.67 03/04/2012 TRAIN 22M 1 M 12 2 12.17 03/04/2012 DEV 23F 1 F 12 2 12.17 03/04/2012 TRAIN 24F 1 F 8 7 8.58 03/04/2012 TRAIN 25M 1 M 10 0 10 03/04/2012 TRAIN 26F 1 F 7 11 7.92 04/04/2012 TEST 27M 1 M 11 5 11.42 10/04/2012 TRAIN 28F 1 F 7 3 7.25 10/04/2012 TRAIN 29F 1 F 7 11 7.92 13/04/2012 TRAIN 30F 1 F 10 5 10.42 16/04/2012 TEST 31F 2 F 9 6 9.5 27/04/2012 TRAIN 32F 2 F 11 8 11.67 02/05/2012 DEV 33F 2 F 8 8 8.67 04/05/2012 TRAIN 34M 2 M 9 11 9.92 24/05/2012 TRAIN 35M 2 M 7 11 7.92 25/05/2012 TRAIN 36M 2 M 10 7 10.58 30/05/2012 TRAIN 37M 2 M 8 7 8.58 31/05/2012 DEV 38M 2 M 9 6 9.5 01/06/2012 TEST 39F 2 F 10 8 10.67 01/06/2012 TEST 40M 2 M 9 1 9.08 05/06/2012 TRAIN 41F 2 F 7 6 7.5 07/06/2012 TRAIN 42M 2 M 8 11 8.92 08/06/2012 TRAIN 43F 2 F 10 1 10.08 14/06/2012 TRAIN 44F 2 F 7 11 7.92 15/06/2012 TRAIN 45M 2 M 10 5 10.42 15/06/2012 TEST 46F 2 F 6 11 6.92 18/06/2012 TRAIN 47M 2 M 11 8 11.67 25/06/2012 TEST 48F 2 F 10 10 10.83 04/07/2012 TRAIN 49F 2 F 8 9 8.75 03/08/2012 TRAIN 50F 2 F 10 6 10.5 03/10/2012 TRAIN 51M 2 M 7 7 7.58 03/10/2012 TRAIN 52F 2 F 8 3 8.25 05/10/2012 TEST 53F 2 F 6 0 6.0 05/10/2012 TEST 54F 2 F 8 2 8.17 09/10/2012 TRAIN 55M 2 M 7 5 7.42 12/10/2012 TEST 56M 2 M 6 9 6.75 16/10/2012 TRAIN 57F 2 F 7 5 7.42 16/10/2012 DEV 58F 2 F 9 1 9.08 17/10/2012 DEV","title":"UXTD - Ultrax Typically Developing Children"},{"location":"data/uxtd-spk/#uxtd-ultrax-typically-developing-children","text":"","title":"UXTD - Ultrax Typically Developing Children"},{"location":"data/uxtd-spk/#a-dataset-of-ultrasound-and-audio-recordings-from-typically-developing-children","text":"Speaker list of UltraSuite's UXTD dataset.","title":"A dataset of ultrasound and audio recordings from typically developing children"},{"location":"data/uxtd-spk/#speakers","text":"SPEAKER-ID GROUP GENDER AGE-Y AGE-M AGE RECORDING DATE SUBSET 01M 1 M 11 10 11.83 13/11/2011 TRAIN 02M 1 M 11 9 11.75 22/12/2011 TRAIN 03F 1 F 10 5 10.42 27/01/2011 TRAIN 04M 1 M 8 9 8.75 27/01/2011 TRAIN 05M 1 M 9 11 9.92 03/02/2012 TRAIN 06F 1 F 9 10 9.83 14/02/2012 TRAIN 07F 1 F 9 4 9.33 14/02/2012 TEST 08M 1 M 8 8 8.67 16/02/2012 TEST 09F 1 F 6 9 6.75 16/02/2012 TRAIN 10F 1 F 11 3 11.25 17/02/2012 TRAIN 11M 1 M 8 1 8.08 09/03/2012 TRAIN 12M 1 M 6 8 6.67 15/03/2012 TEST 13F 1 F 11 7 11.58 21/03/2012 TEST 14M 1 M 12 4 12.33 21/03/2012 TRAIN 15M 1 M 7 11 7.92 21/03/2012 TRAIN 16F 1 F 12 10 12.83 21/03/2012 TRAIN 17M 1 M 10 8 10.67 21/02/2012 DEV 18F 1 F 7 2 7.17 30/03/2012 TRAIN 19M 1 M 12 6 12.5 30/03/2012 TRAIN 20M 1 M 11 1 11.08 02/04/2012 TRAIN 21F 1 F 5 8 5.67 03/04/2012 TRAIN 22M 1 M 12 2 12.17 03/04/2012 DEV 23F 1 F 12 2 12.17 03/04/2012 TRAIN 24F 1 F 8 7 8.58 03/04/2012 TRAIN 25M 1 M 10 0 10 03/04/2012 TRAIN 26F 1 F 7 11 7.92 04/04/2012 TEST 27M 1 M 11 5 11.42 10/04/2012 TRAIN 28F 1 F 7 3 7.25 10/04/2012 TRAIN 29F 1 F 7 11 7.92 13/04/2012 TRAIN 30F 1 F 10 5 10.42 16/04/2012 TEST 31F 2 F 9 6 9.5 27/04/2012 TRAIN 32F 2 F 11 8 11.67 02/05/2012 DEV 33F 2 F 8 8 8.67 04/05/2012 TRAIN 34M 2 M 9 11 9.92 24/05/2012 TRAIN 35M 2 M 7 11 7.92 25/05/2012 TRAIN 36M 2 M 10 7 10.58 30/05/2012 TRAIN 37M 2 M 8 7 8.58 31/05/2012 DEV 38M 2 M 9 6 9.5 01/06/2012 TEST 39F 2 F 10 8 10.67 01/06/2012 TEST 40M 2 M 9 1 9.08 05/06/2012 TRAIN 41F 2 F 7 6 7.5 07/06/2012 TRAIN 42M 2 M 8 11 8.92 08/06/2012 TRAIN 43F 2 F 10 1 10.08 14/06/2012 TRAIN 44F 2 F 7 11 7.92 15/06/2012 TRAIN 45M 2 M 10 5 10.42 15/06/2012 TEST 46F 2 F 6 11 6.92 18/06/2012 TRAIN 47M 2 M 11 8 11.67 25/06/2012 TEST 48F 2 F 10 10 10.83 04/07/2012 TRAIN 49F 2 F 8 9 8.75 03/08/2012 TRAIN 50F 2 F 10 6 10.5 03/10/2012 TRAIN 51M 2 M 7 7 7.58 03/10/2012 TRAIN 52F 2 F 8 3 8.25 05/10/2012 TEST 53F 2 F 6 0 6.0 05/10/2012 TEST 54F 2 F 8 2 8.17 09/10/2012 TRAIN 55M 2 M 7 5 7.42 12/10/2012 TEST 56M 2 M 6 9 6.75 16/10/2012 TRAIN 57F 2 F 7 5 7.42 16/10/2012 DEV 58F 2 F 9 1 9.08 17/10/2012 DEV","title":"Speakers"},{"location":"data/uxtd/","text":"Ultrax Typically Developing Children A dataset of ultrasound and audio recordings from typically developing children Speakers The UXTD dataset contains 58 speakers (31 female and 27 male), aged 5-12 years. For a list and additional details, see UXTD Speakers . Data Types Core data types Data type Description wav speech waveform ult raw ultrasound data param ultrasound parameters txt prompt text with date/time of utterance recording Additional data Data type Description transcriptions transcription for utterances of type X and X. slt_labels manual annotation from SLT, when available. See [2] for details speaker_labels speaker diarization identifying therapist (SLT) and child (CHILD) speech word_labels automatic word-level alignment phone_labels automatic phone-level alignment reference_labels manually-revised labels (see below for details) Labels are available in Praat's TextGrid format and HTK's lab format. Speaker, word, and phone labels were generated according to the methods described in [3]. File IDs Individual recordings are indexed for each session according to their recording times. See the prompt text file for recording date/time. Each file ID also includes a prompt type identifier. See Data for details. Reference Labels Reference labels are given for a few utterances of the UXTD and UXSSD datasets. These have been manually revised at the speaker (60 utterances) and word level (199 utterances). The revision was done by a single annotator. Note that phone labels are also provided, but these are not entirely manually-revised . This set of annotation is force-aligned at the phone level, but constrained by the manually-revised word boundaries. Labels are available in Praat's TextGrid format ( TG ) and HTK's lab format ( lab ). The structure for the directory is as follows: /uxtd /phone_labels /lab /TG /word_labels /lab /TG /speaker_labels /lab /TG References [1] Eshky, A., Ribeiro, M. S., Cleland, J., Richmond, K., Roxburgh, Z., Scobbie, J., & Wrench, A. (2018) Ultrasuite: A repository of ultrasound and acoustic data from child speech therapy sessions . Proceedings of INTERSPEECH. Hyderabad, India. [2] Cleland, J., Scobbie, J., Naki, S., & Wrench, A. (2015). Helping children learn non-native articulations: the implications for ultrasound-based clinical intervention. Proceedings of the 18th International Congress of Phonetic Sciences : ICPhS 2015. ed. / The Scottish Consortium for ICPhS 2015. 1. ed. Scotland, 2015. p. 1-5 698. [3] Ribeiro, M. S., Eshky, A., Richmond, K., Renals, S., (2019). Ultrasound tongue imaging for diarization and alignment of child speech therapy sessions . Proceedings of INTERSPEECH. Graz, Austria.","title":"Ultrax Typically Developing"},{"location":"data/uxtd/#ultrax-typically-developing-children","text":"A dataset of ultrasound and audio recordings from typically developing children","title":"Ultrax Typically Developing Children"},{"location":"data/uxtd/#speakers","text":"The UXTD dataset contains 58 speakers (31 female and 27 male), aged 5-12 years. For a list and additional details, see UXTD Speakers .","title":"Speakers"},{"location":"data/uxtd/#data-types","text":"Core data types Data type Description wav speech waveform ult raw ultrasound data param ultrasound parameters txt prompt text with date/time of utterance recording Additional data Data type Description transcriptions transcription for utterances of type X and X. slt_labels manual annotation from SLT, when available. See [2] for details speaker_labels speaker diarization identifying therapist (SLT) and child (CHILD) speech word_labels automatic word-level alignment phone_labels automatic phone-level alignment reference_labels manually-revised labels (see below for details) Labels are available in Praat's TextGrid format and HTK's lab format. Speaker, word, and phone labels were generated according to the methods described in [3].","title":"Data Types"},{"location":"data/uxtd/#file-ids","text":"Individual recordings are indexed for each session according to their recording times. See the prompt text file for recording date/time. Each file ID also includes a prompt type identifier. See Data for details.","title":"File IDs"},{"location":"data/uxtd/#reference-labels","text":"Reference labels are given for a few utterances of the UXTD and UXSSD datasets. These have been manually revised at the speaker (60 utterances) and word level (199 utterances). The revision was done by a single annotator. Note that phone labels are also provided, but these are not entirely manually-revised . This set of annotation is force-aligned at the phone level, but constrained by the manually-revised word boundaries. Labels are available in Praat's TextGrid format ( TG ) and HTK's lab format ( lab ). The structure for the directory is as follows: /uxtd /phone_labels /lab /TG /word_labels /lab /TG /speaker_labels /lab /TG","title":"Reference Labels"},{"location":"data/uxtd/#references","text":"[1] Eshky, A., Ribeiro, M. S., Cleland, J., Richmond, K., Roxburgh, Z., Scobbie, J., & Wrench, A. (2018) Ultrasuite: A repository of ultrasound and acoustic data from child speech therapy sessions . Proceedings of INTERSPEECH. Hyderabad, India. [2] Cleland, J., Scobbie, J., Naki, S., & Wrench, A. (2015). Helping children learn non-native articulations: the implications for ultrasound-based clinical intervention. Proceedings of the 18th International Congress of Phonetic Sciences : ICPhS 2015. ed. / The Scottish Consortium for ICPhS 2015. 1. ed. Scotland, 2015. p. 1-5 698. [3] Ribeiro, M. S., Eshky, A., Richmond, K., Renals, S., (2019). Ultrasound tongue imaging for diarization and alignment of child speech therapy sessions . Proceedings of INTERSPEECH. Graz, Austria.","title":"References"}]}